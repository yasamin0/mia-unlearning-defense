{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef61da1f-bc68-4fad-86c3-c11c6fae4574",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n",
    "\n",
    "In this step, we import the necessary libraries from PyTorch and Torchvision for building and training our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d05ec4-3695-42af-9bb0-f949bff392ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796be0e6-8632-45c8-b3fa-ab391d0cc69f",
   "metadata": {},
   "source": [
    "### Step 2: Load and Prepare the MNIST Dataset\n",
    "\n",
    "We download the MNIST handwritten digits dataset, convert the images to tensors, normalize them, and prepare them in batches using DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa86047-7f8e-4884-8fa9-8f59e775f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Train size:\", len(trainset))\n",
    "print(\"Test size:\", len(testset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635538b-4950-4954-bd83-7934a78b2f57",
   "metadata": {},
   "source": [
    "### Step 3: Define the Convolutional Neural Network (CNN)\n",
    "\n",
    "Here, we define a simple CNN with two convolutional layers followed by fully connected layers to classify input images into 10 digit classes (0–9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca4da50-437c-4ffd-809d-3617ee038bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5eb3c0-f893-4a71-ab4e-35b20ed2dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()              # چون خروجی ما 10 کلاس هست (0 تا 9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # الگوریتم Adam برای بهینه‌سازی\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95a967-2156-4724-af82-9b8f9a372b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74e09ce3-1d9c-4cab-b47d-9924873cbfce",
   "metadata": {},
   "source": [
    "### Step 4: Define Loss Function and Optimizer\n",
    "\n",
    "We use `CrossEntropyLoss` since this is a multi-class classification problem.  \n",
    "The Adam optimizer will update the model weights to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d790133-b51a-4cc4-8acd-5f6de675ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification (digits 0–9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78f971-9deb-410c-942d-6f9c0ec2a686",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model\n",
    "\n",
    "We train the CNN for 5 epochs on the MNIST training data.  \n",
    "After each epoch, we print the training loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6faaf0e1-be86-4266-b697-ac2d4bdaaf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 146.287 - Accuracy: 95.35%\n",
      "Epoch 2 - Loss: 43.185 - Accuracy: 98.57%\n",
      "Epoch 3 - Loss: 28.295 - Accuracy: 99.04%\n",
      "Epoch 4 - Loss: 21.075 - Accuracy: 99.26%\n",
      "Epoch 5 - Loss: 15.996 - Accuracy: 99.44%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss:.3f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29dee7-fde4-4ce7-9b75-3be2eb6b1bb2",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate on Test Set\n",
    "\n",
    "Now we check how well the model performs on unseen data.  \n",
    "This helps us understand its generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee88892-0129-4e7f-87f8-89d6e3c482f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.95%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99563bd0-077a-4842-bd96-3931d2bb49c9",
   "metadata": {},
   "source": [
    "### Step 7: Save the Trained Model\n",
    "\n",
    "We save the trained model's weights in a `.pth` file for later use.  \n",
    "This will allow us to reload it for attacks or unlearning experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6994c9ec-c739-4c96-aa57-3c0d314667b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/cnn_mnist.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a49abb-e044-4aae-822d-23a5b0e997cd",
   "metadata": {},
   "source": [
    "### Step 8: Compute Model Confidence for Each Input\n",
    "\n",
    "We pass both training and test samples through the model and use softmax to convert the logits into probabilities.  \n",
    "Then, we extract the maximum confidence value (highest softmax score) for each input image.  \n",
    "These values help us understand how confident the model is for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a2ec338-98ef-4b1e-97dd-604c1e5ec68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example confidences (train): [0.99999857, 1.0, 0.9991178, 0.9999968, 0.9998772]\n",
      "Example confidences (test):  [0.99999857, 1.0, 0.99996364, 0.9999999, 0.9999993]\n"
     ]
    }
   ],
   "source": [
    "def get_confidences(model, dataloader):\n",
    "    confidences = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)              # Convert logits to probabilities\n",
    "            max_conf, _ = torch.max(probs, dim=1)              # Get max probability (confidence)\n",
    "            confidences.extend(max_conf.cpu().numpy())         # Store confidence values\n",
    "    return confidences\n",
    "\n",
    "# Get confidence values for train and test sets\n",
    "train_conf = get_confidences(model, trainloader)\n",
    "test_conf = get_confidences(model, testloader)\n",
    "\n",
    "print(f\"Example confidences (train): {train_conf[:5]}\")\n",
    "print(f\"Example confidences (test):  {test_conf[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95e9db-e364-4df0-8e18-f60232243e67",
   "metadata": {},
   "source": [
    "### Step 8: Save Confidence Scores for Later Analysis\n",
    "\n",
    "We save the extracted confidence values from the training and test sets into `.npy` files.  \n",
    "This allows us to re-use the data later without having to re-run inference, for example in unlearning or plotting analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff803db3-8974-4ac1-9ed9-23ff560f713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence logs saved successfully to './results/'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "np.save('./results/train_confidences.npy', np.array(train_conf))\n",
    "np.save('./results/test_confidences.npy', np.array(test_conf))\n",
    "\n",
    "print(\"Confidence logs saved successfully to './results/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8090d-6728-4904-b76d-a44613b0c8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
